{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfEjrt1dBDAe",
        "outputId": "1fc9c41e-6417-444b-fa60-4adffc76badc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")  # For embeddings\n",
        "classifier_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # For classification\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "bert_model.eval()\n",
        "classifier_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICSumXy7BMp0",
        "outputId": "b0128afe-c2de-4fc0-d1ae-112bdc7aa263"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"white hat\"  # Example sentence\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Inputs for embedding extraction\n",
        "input_ids = inputs[\"input_ids\"]  # Token IDs\n",
        "attention_mask = inputs[\"attention_mask\"]  # Attention mask"
      ],
      "metadata": {
        "id": "FQ9xJ95ZBR-m"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "    contextual_embeddings = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_size]\n",
        "    # Original text embeddings: [1, 11, 768]\n",
        "    pooled_text_embedding = contextual_embeddings.mean(dim=1)  # Shape: [1, 768]\n",
        "\n",
        "\n",
        "print(f\"pooled_text_embedding.shape: {pooled_text_embedding.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmYVbP26BVZ6",
        "outputId": "a05a150c-6d73-444f-d41f-6b88bcbe0b76"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pooled_text_embedding.shape: torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = classifier_model(input_ids, attention_mask=attention_mask).logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Interpret the result\n",
        "if predictions.item() == 1:\n",
        "    print(\"The sentence contains an idiom.\")\n",
        "else:\n",
        "    print(\"The sentence does not contain an idiom.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byy48popBcq9",
        "outputId": "8d199d30-626e-4f32-b982-ebc547cd8ef6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence does not contain an idiom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGAUYBHWOMox",
        "outputId": "8f1f088c-dfd3-4a84-cdcf-e88dd2637365"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "from PIL import Image\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ErpNid7tPCI1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ViT model and feature extractor\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwC2mW6hQVDY",
        "outputId": "19660077-c8f4-4000-d727-e374370e9164"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTModel(\n",
              "  (embeddings): ViTEmbeddings(\n",
              "    (patch_embeddings): ViTPatchEmbeddings(\n",
              "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): ViTEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x ViTLayer(\n",
              "        (attention): ViTSdpaAttention(\n",
              "          (attention): ViTSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (pooler): ViTPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder containing images\n",
        "folder_path = \"/content\"\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('png', 'jpg', 'jpeg'))]\n",
        "print(image_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK85Ic7fQbh5",
        "outputId": "edf193ef-22c1-438b-d4f0-2dbf5e7ec2db"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/printer.png', '/content/hat.png', '/content/idiomatic.png', '/content/Literal.png', '/content/idiomatic2.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images\n",
        "images = [Image.open(file).convert(\"RGB\") for file in image_files]  # Open and convert to RGB\n",
        "inputs = feature_extractor(images=images, return_tensors=\"pt\")  # Batch preprocessing\n"
      ],
      "metadata": {
        "id": "wjYdK2ENRRrq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the preprocessed images through the ViT model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract the [CLS] token embeddings for all images\n",
        "cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "print(f\"CLS Embeddings Shape: {cls_embeddings.shape}\")  # Shape: [5, 768] for 5 images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8I0wVakRe74",
        "outputId": "fe00988f-ba87-4e34-cf63-0f1e1ffea5c9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS Embeddings Shape: torch.Size([5, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjuj44HliEg9",
        "outputId": "38ff6614-68ef-4c9e-fec3-ba59fc1c7334"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define a linear projection layer to reduce dimensions from 768 to 512\n",
        "linear_projection = nn.Linear(768, 512)\n",
        "\n",
        "# Apply the linear transformation\n",
        "cls_embeddings_reduced = linear_projection(cls_embeddings)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(f\"Original Shape: {cls_embeddings.shape}\")  # Output: [5, 768]\n",
        "print(f\"Reduced Shape: {cls_embeddings_reduced.shape}\")  # Output: [5, 512]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttp478SriHzK",
        "outputId": "706f5c0b-2202-4ee3-db4b-0ad47ded105a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shape: torch.Size([5, 768])\n",
            "Reduced Shape: torch.Size([5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the .pt file\n",
        "file_path = \"/content/white_hat.pt\"\n",
        "\n",
        "# Load the embeddings\n",
        "data = torch.load(file_path)\n",
        "\n",
        "# Check the type of the loaded data\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYAEPT9sSpu",
        "outputId": "582d31b3-0dbd-4588-99ca-cd0c1e3f82d2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-bfe6f00b92cc>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence embedding: Shape [1, 512]\n",
        "sentence_embedding = torch.tensor(data) # Replace with your actual embedding\n",
        "\n",
        "# Example image embeddings: Shape [5, 512]\n",
        "image_embeddings = cls_embeddings_reduced  # Replace with your actual embeddings\n",
        "\n",
        "# Step 1: Compute cosine similarity\n",
        "# Cosine similarity is computed separately for each image embedding\n",
        "similarities = F.cosine_similarity(sentence_embedding, image_embeddings, dim=-1)  # Shape: [5]\n",
        "\n",
        "# Step 2: Rank similarities\n",
        "# Sort indices of the images based on similarity scores (descending order)\n",
        "ranked_indices = torch.argsort(similarities, descending=True)\n",
        "\n",
        "# Print results\n",
        "print(\"Similarity scores:\", similarities)  # Shape: [5]\n",
        "print(\"Ranked indices:\", ranked_indices)   # Indices sorted by similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLc1CKfTsjgo",
        "outputId": "21570a33-aed7-455f-c3cc-e6957b9d6908"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity scores: tensor([ 0.0471, -0.0222, -0.0402, -0.0177, -0.0599], grad_fn=<SumBackward1>)\n",
            "Ranked indices: tensor([0, 3, 1, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example sentence embedding: Shape [1, 768]\n",
        "sentence_embedding = pooled_text_embedding  # Replace with your actual embedding\n",
        "\n",
        "# Example image embeddings: Shape [5, 768]\n",
        "image_embeddings = cls_embeddings  # Replace with your actual embeddings\n",
        "\n",
        "# Step 1: Compute cosine similarity\n",
        "# Broadcast sentence embedding to match image embeddings\n",
        "similarities = F.cosine_similarity(sentence_embedding, image_embeddings, dim=-1)  # Shape: [5]\n",
        "\n",
        "# Step 2: Rank similarities\n",
        "ranked_indices = torch.argsort(similarities, descending=True)  # Indices of images sorted by similarity\n",
        "\n",
        "# Print results\n",
        "print(\"Similarity scores:\", similarities)\n",
        "print(\"Ranked indices:\", ranked_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJXZ-D_MR7g4",
        "outputId": "0d650491-e7d8-41b3-e134-de21e69e09f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity scores: tensor([-0.0189,  0.0437,  0.0418, -0.0418, -0.0515])\n",
            "Ranked indices: tensor([1, 2, 0, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTv7KpKpTis6",
        "outputId": "bc2608e1-1ccd-44ba-a1f7-f68d9f66ccdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distances: tensor([[10.0676, 10.1501,  9.6485,  9.8801, 10.6379]])\n",
            "Ranked indices: tensor([2, 3, 0, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h_hqF6rUeui",
        "outputId": "751035d0-e907-4bd3-ffb3-44d4ccf5d8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot Products: tensor([-2.0198,  2.0034,  1.6785, -0.7186, -2.0665])\n",
            "Ranked indices: tensor([1, 2, 3, 0, 4])\n"
          ]
        }
      ]
    }
  ]
}