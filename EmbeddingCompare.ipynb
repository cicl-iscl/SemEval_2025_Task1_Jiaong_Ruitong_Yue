{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfEjrt1dBDAe",
        "outputId": "65771731-5014-4bc9-a661-eeebe9ba2e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")  # For embeddings\n",
        "classifier_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # For classification\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "bert_model.eval()\n",
        "classifier_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICSumXy7BMp0",
        "outputId": "dfcce525-f168-477c-98ad-35816d4e8b06"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"white hat\"  # Example sentence\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Inputs for embedding extraction\n",
        "input_ids = inputs[\"input_ids\"]  # Token IDs\n",
        "attention_mask = inputs[\"attention_mask\"]  # Attention mask"
      ],
      "metadata": {
        "id": "FQ9xJ95ZBR-m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "    contextual_embeddings = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_size]\n",
        "    # Original text embeddings: [1, 11, 768]\n",
        "    pooled_text_embedding = contextual_embeddings.mean(dim=1)  # Shape: [1, 768]\n",
        "\n",
        "\n",
        "print(f\"pooled_text_embedding.shape: {pooled_text_embedding.shape}\")\n",
        "print(type(pooled_text_embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmYVbP26BVZ6",
        "outputId": "c5b630b0-e234-4737-fbd0-a02e4cf648dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pooled_text_embedding.shape: torch.Size([1, 768])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = classifier_model(input_ids, attention_mask=attention_mask).logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Interpret the result\n",
        "if predictions.item() == 1:\n",
        "    print(\"The sentence contains an idiom.\")\n",
        "else:\n",
        "    print(\"The sentence does not contain an idiom.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byy48popBcq9",
        "outputId": "d5b483c8-d40b-4c4e-dd7a-d96199e9e333"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence contains an idiom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGAUYBHWOMox",
        "outputId": "72c36c82-1a15-4d01-d5fa-09747eeae763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "from PIL import Image\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ErpNid7tPCI1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ViT model and feature extractor\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwC2mW6hQVDY",
        "outputId": "febde271-1f1d-45b3-d5a9-55ba44584673"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTModel(\n",
              "  (embeddings): ViTEmbeddings(\n",
              "    (patch_embeddings): ViTPatchEmbeddings(\n",
              "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): ViTEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x ViTLayer(\n",
              "        (attention): ViTSdpaAttention(\n",
              "          (attention): ViTSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): ViTSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): ViTIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): ViTOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (pooler): ViTPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文件夹路径 file path\n",
        "folder_path = \"/content\"\n",
        "\n",
        "# 列出所有图片文件 all the image under the folder\n",
        "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('png', 'jpg', 'jpeg'))]\n",
        "\n",
        "# 获取图片ID（文件名）get the image id of each image\n",
        "image_ids = [os.path.basename(file).split('.')[0] for file in image_files]\n",
        "\n",
        "print(image_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK85Ic7fQbh5",
        "outputId": "1e1e4ea8-10a1-444a-ad09-78141433b5bd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['50305046415', '11696820520', '12124292214', '39481587509', '13755461305']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images\n",
        "images = [Image.open(file).convert(\"RGB\") for file in image_files]\n",
        "inputs = feature_extractor(images=images, return_tensors=\"pt\")  # Batch preprocessing\n"
      ],
      "metadata": {
        "id": "wjYdK2ENRRrq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the preprocessed images through the ViT model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "print(f\"CLS Embeddings Shape: {cls_embeddings.shape}\")  # Shape: [5, 768] for 5 images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8I0wVakRe74",
        "outputId": "7b3816d2-f8af-4bb5-980e-35393fbe265f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS Embeddings Shape: torch.Size([5, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成图片 ID 和对应的 CLS embedding\n",
        "id_embedding_map = {image_id: image_embedding for image_id, image_embedding in zip(image_ids, cls_embeddings)}\n",
        "\n",
        "# 打印每个图片 ID 和其对应的 embedding 形状\n",
        "for image_id, image_emb in id_embedding_map.items():\n",
        "    print(f\"Image ID: {image_id}, Image Emb Shape: {image_emb.shape}\")\n",
        "    print(f\"Pooled Text Emb Shape: {pooled_text_embedding.shape}\")\n",
        "\n",
        "    # 修复零维 embedding\n",
        "    if image_emb.dim() == 0:\n",
        "        image_emb = image_emb.unsqueeze(0)  # 添加一个维度\n",
        "        print(f\"Fixed Image Emb Shape for ID {image_id}: {image_emb.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJcszv9gmCDP",
        "outputId": "05487a31-507d-4a6b-a3f6-24b15f61e6f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image ID: 50305046415, Image Emb Shape: torch.Size([768])\n",
            "Pooled Text Emb Shape: torch.Size([768])\n",
            "Image ID: 11696820520, Image Emb Shape: torch.Size([768])\n",
            "Pooled Text Emb Shape: torch.Size([768])\n",
            "Image ID: 12124292214, Image Emb Shape: torch.Size([768])\n",
            "Pooled Text Emb Shape: torch.Size([768])\n",
            "Image ID: 39481587509, Image Emb Shape: torch.Size([768])\n",
            "Pooled Text Emb Shape: torch.Size([768])\n",
            "Image ID: 13755461305, Image Emb Shape: torch.Size([768])\n",
            "Pooled Text Emb Shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# define the nerual network model\n",
        "class VectorComparisonNet(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(VectorComparisonNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, image_emb, text_emb):\n",
        "        # concat the features\n",
        "        combined = torch.cat([image_emb, text_emb], dim=-1)\n",
        "        hidden = self.relu(self.fc1(combined))\n",
        "        output = self.sigmoid(self.fc2(hidden))\n",
        "        return output\n",
        "\n",
        "embedding_dim = 768  # image and text embedding dimension\n",
        "hidden_dim = 128     # hidden layer dimension\n",
        "model = VectorComparisonNet(embedding_dim, hidden_dim)\n",
        "\n",
        "# pooling\n",
        "pooled_text_embedding = pooled_text_embedding.squeeze(0)\n",
        "\n",
        "results = []\n",
        "for image_id, image_emb in id_embedding_map.items():\n",
        "    # forward computing probability\n",
        "    probability = model(image_emb, pooled_text_embedding)  # input the model\n",
        "    results.append((image_id, probability.item()))  # save result\n",
        "\n",
        "# print the result\n",
        "for image_id, prob in results:\n",
        "    print(f\"Image ID: {image_id}, Match Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjuj44HliEg9",
        "outputId": "41bc25f9-4766-45f9-a0c3-a705410a2628"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image ID: 50305046415, Match Probability: 0.4602\n",
            "Image ID: 11696820520, Match Probability: 0.4556\n",
            "Image ID: 12124292214, Match Probability: 0.4629\n",
            "Image ID: 39481587509, Match Probability: 0.4583\n",
            "Image ID: 13755461305, Match Probability: 0.4636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define a linear projection layer to reduce dimensions from 768 to 512\n",
        "linear_projection = nn.Linear(768, 512)\n",
        "\n",
        "# Apply the linear transformation\n",
        "cls_embeddings_reduced = linear_projection(cls_embeddings)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(f\"Original Shape: {cls_embeddings.shape}\")  # Output: [5, 768]\n",
        "print(f\"Reduced Shape: {cls_embeddings_reduced.shape}\")  # Output: [5, 512]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttp478SriHzK",
        "outputId": "706f5c0b-2202-4ee3-db4b-0ad47ded105a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shape: torch.Size([5, 768])\n",
            "Reduced Shape: torch.Size([5, 512])\n"
          ]
        }
      ]
    }
  ]
}