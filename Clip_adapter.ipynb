{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no7-PxCuh0UO",
        "outputId": "5a1fb7df-889a-451a-b192-184c6f436a69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPModel\n",
        "\n",
        "# Load the CLIP model\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# Load the embedding dictionary and golden truth JSON\n",
        "with open(\"/content/drive/MyDrive/golden_truth/golden_truth_idiomatic.json\", \"r\") as gt_file:\n",
        "    golden_truth = json.load(gt_file)\n",
        "\n",
        "# Load the embedding dictionary\n",
        "embedding_dict_path = \"/content/drive/MyDrive/clip_train_idiom_embeddings.pt\"\n",
        "embedding_dict = torch.load(embedding_dict_path)\n",
        "\n",
        "print(type(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1QrT_4voYMJ",
        "outputId": "6312145c-8a61-4f9f-a9c0-2c4710ddc5dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-2bbe7604aaa7>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  embedding_dict = torch.load(embedding_dict_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(embedding_dict[0]))  # Print the first element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps3uYQFRtveP",
        "outputId": "02587d7f-1c5c-4623-9626-d324f89d3576"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the structure of the first dictionary in the list\n",
        "for i, compound_data in enumerate(embedding_dict[:1]):  # Print details of the first compound\n",
        "    print(f\"Compound {i + 1}: {compound_data['compound_name']}\")\n",
        "    print(f\"Text Embedding Shape: {compound_data['text_embedding'].shape}\")\n",
        "    print(f\"Number of Images: {len(compound_data['images'])}\")\n",
        "    for img in compound_data['images']:\n",
        "        print(f\"  Image ID: {img['image_id']}, Image Embedding Shape: {img['image_embedding'].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDKzu4Ueuq1p",
        "outputId": "cbdbea8c-7062-4c3a-b417-84d3914b5ded"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compound 1: elbow grease\n",
            "Text Embedding Shape: torch.Size([1, 512])\n",
            "Number of Images: 5\n",
            "  Image ID: 74852536462.png, Image Embedding Shape: torch.Size([1, 512])\n",
            "  Image ID: 53378381715.png, Image Embedding Shape: torch.Size([1, 512])\n",
            "  Image ID: 39938261459.png, Image Embedding Shape: torch.Size([1, 512])\n",
            "  Image ID: 54879908369.png, Image Embedding Shape: torch.Size([1, 512])\n",
            "  Image ID: 35234427395.png, Image Embedding Shape: torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = embedding_dict['compound_name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "1BSTfoHKxceA",
        "outputId": "50aa7c82-00ad-4d14-acec-bf74af34ef21"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'compound_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-ebb29f38b16b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'compound_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, embedding_dict, golden_truth):\n",
        "        \"\"\"\n",
        "        :param embedding_dict: Precomputed embeddings for text and images\n",
        "        :param golden_truth: JSON dictionary with the correct ranking for each compound\n",
        "        \"\"\"\n",
        "        self.embedding_dict = embedding_dict\n",
        "        self.golden_truth = golden_truth\n",
        "        self.compounds = list(golden_truth.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.compounds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        compound = self.compounds[idx]\n",
        "        print(f\"\\nProcessing Compound: {compound}\")\n",
        "\n",
        "        # Check if compound exists in the embedding dictionary\n",
        "        if compound not in self.embedding_dict:\n",
        "            print(f\"Compound '{compound}' not found in embedding dictionary. Skipping.\")\n",
        "            return None  # Skip missing entries\n",
        "\n",
        "        # Access text embedding and image embeddings\n",
        "        data = self.embedding_dict[compound]\n",
        "        text_embedding = data[\"text_embedding\"]  # Shape: (1, 512)\n",
        "        print(f\"Text Embedding Shape: {text_embedding.shape}\")\n",
        "\n",
        "        images = data[\"images\"]\n",
        "        print(f\"Number of Images: {len(images)}\")\n",
        "        image_embeddings = torch.stack([img[\"image_embedding\"] for img in images])  # Shape: (num_images, 512)\n",
        "        print(f\"Image Embeddings Shape: {image_embeddings.shape}\")\n",
        "\n",
        "        image_ids = [img[\"image_id\"] for img in images]\n",
        "        print(f\"Image IDs: {image_ids}\")\n",
        "\n",
        "        # Map golden truth rank to indices\n",
        "        correct_order = self.golden_truth.get(compound, [])\n",
        "        print(f\"Correct Order from Golden Truth: {correct_order}\")\n",
        "\n",
        "        correct_indices = [\n",
        "            image_ids.index(img_id) for img_id in correct_order if img_id in image_ids\n",
        "        ]\n",
        "        print(f\"Correct Indices: {correct_indices}\")\n",
        "\n",
        "        return text_embedding.squeeze(0), image_embeddings, correct_indices\n",
        "\n",
        "dataset = RankingDataset(embedding_dict, golden_truth)\n",
        "\n",
        "# Iterate over the dataset and print the debug output for the first 2 compounds\n",
        "for idx in range(2):\n",
        "    result = dataset[idx]\n",
        "    if result:\n",
        "        text_embedding, image_embeddings, correct_indices = result\n",
        "        print(f\"Processed Result for Compound {idx + 1}:\")\n",
        "        print(f\"  Text Embedding Shape: {text_embedding.shape}\")\n",
        "        print(f\"  Image Embeddings Shape: {image_embeddings.shape}\")\n",
        "        print(f\"  Correct Indices: {correct_indices}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "G2oEhFkPv_i-",
        "outputId": "dacbd3f7-43ce-4497-f07a-ea5f6475a283"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Compound: elbow grease\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-59aaf8030fab>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Iterate over the dataset and print the debug output for the first 2 compounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-59aaf8030fab>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Access text embedding and image embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompound\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (1, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Text Embedding Shape: {text_embedding.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the CLIP Adapter\n",
        "class CLIPAdapter(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CLIPAdapter, self).__init__()\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)  # Predict a single score per image embedding\n",
        "        )\n",
        "\n",
        "    def forward(self, text_embedding, image_embeddings):\n",
        "        # Normalize embeddings\n",
        "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute dot product similarity scores\n",
        "        similarity_scores = torch.matmul(image_embeddings, text_embedding.unsqueeze(-1)).squeeze(-1)\n",
        "        # Refine scores using the adapter\n",
        "        refined_scores = self.adapter(similarity_scores.unsqueeze(-1)).squeeze(-1)\n",
        "        return refined_scores\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, embedding_dict, golden_truth):\n",
        "        \"\"\"\n",
        "        :param embedding_dict: Precomputed embeddings for text and images\n",
        "        :param golden_truth: JSON dictionary with the correct ranking for each compound\n",
        "        \"\"\"\n",
        "        self.embedding_dict = embedding_dict\n",
        "        self.golden_truth = golden_truth\n",
        "        self.compounds = list(golden_truth.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.compounds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        compound = self.compounds[idx]\n",
        "\n",
        "        # Check if compound exists in the embedding dictionary\n",
        "        if compound not in self.embedding_dict:\n",
        "            return None  # Skip missing entries\n",
        "\n",
        "        # Access text embedding and image embeddings\n",
        "        data = self.embedding_dict[compound]\n",
        "        text_embedding = data[\"text_embedding\"]  # Shape: (1, 512)\n",
        "        images = data[\"images\"]\n",
        "        image_embeddings = torch.stack([img[\"image_embedding\"] for img in images])  # Shape: (num_images, 512)\n",
        "        image_ids = [img[\"image_id\"] for img in images]\n",
        "\n",
        "        # Map golden truth rank to indices\n",
        "        correct_order = self.golden_truth.get(compound, [])\n",
        "        correct_indices = [image_ids.index(img_id) for img_id in correct_order if img_id in image_ids]\n",
        "\n",
        "        return text_embedding.squeeze(0), image_embeddings, correct_indices\n",
        "\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]  # Filter out None entries\n",
        "    if not batch:\n",
        "        return None  # Return None if the batch is empty\n",
        "    return torch.utils.data.default_collate(batch)\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def listnet_loss(pred_scores, true_indices):\n",
        "    \"\"\"\n",
        "    Compute the ListNet loss for predicted scores and true rankings.\n",
        "    \"\"\"\n",
        "    num_images = pred_scores.size(0)\n",
        "    true_distribution = torch.zeros(num_images, dtype=torch.float32, device=pred_scores.device)\n",
        "    for i, idx in enumerate(true_indices):\n",
        "        true_distribution[idx] = len(true_indices) - i  # Higher rank gets higher weight\n",
        "    true_distribution = true_distribution / true_distribution.sum()  # Normalize to probabilities\n",
        "    pred_prob = torch.softmax(pred_scores, dim=0)  # Predicted probabilities\n",
        "    loss = -torch.sum(true_distribution * torch.log(pred_prob + 1e-9))  # Cross-entropy loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            if batch is None:  # Skip empty batches\n",
        "                continue\n",
        "\n",
        "            text_embedding, image_embeddings, true_indices = batch\n",
        "            text_embedding = text_embedding.squeeze(0)  # Shape: (512,)\n",
        "            image_embeddings = image_embeddings.squeeze(0)  # Shape: (num_images, 512)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_scores = model(text_embedding, image_embeddings)\n",
        "            loss = listnet_loss(pred_scores, true_indices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Initialize model, optimizer, and dataloader\n",
        "embedding_dim = 512  # Dimension of the embeddings\n",
        "clip_adapter = CLIPAdapter(embedding_dim)\n",
        "optimizer = optim.Adam(clip_adapter.parameters(), lr=1e-4)\n",
        "\n",
        "# Example: Load dataset and golden truth\n",
        "# embedding_dict = torch.load(\"/path/to/embedding_dict.pt\")\n",
        "# golden_truth = json.load(open(\"/path/to/golden_truth.json\"))\n",
        "\n",
        "dataset = RankingDataset(embedding_dict, golden_truth)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Train the model\n",
        "train_model(clip_adapter, dataloader, optimizer, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "CP3ZQws6voCP",
        "outputId": "8980a78a-43fa-44d9-aa40-a946d7efaf52"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-cf0f2ad61246>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_adapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-cf0f2ad61246>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip empty batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-cf0f2ad61246>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Access text embedding and image embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompound\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (1, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (num_images, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the CLIP Adapter\n",
        "class CLIPAdapter(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CLIPAdapter, self).__init__()\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)  # Predict a single score per image embedding\n",
        "        )\n",
        "\n",
        "    def forward(self, text_embedding, image_embeddings):\n",
        "        # Normalize embeddings\n",
        "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute dot product similarity scores\n",
        "        similarity_scores = torch.matmul(image_embeddings, text_embedding.unsqueeze(-1)).squeeze(-1)\n",
        "        # Refine scores using the adapter\n",
        "        refined_scores = self.adapter(similarity_scores.unsqueeze(-1)).squeeze(-1)\n",
        "        return refined_scores\n",
        "\n",
        "# Dataset class\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, embedding_dict, golden_truth):\n",
        "        \"\"\"\n",
        "        :param embedding_dict: Precomputed embeddings for text and images\n",
        "        :param golden_truth: JSON dictionary with the correct ranking for each compound\n",
        "        \"\"\"\n",
        "        self.embedding_dict = embedding_dict\n",
        "        self.golden_truth = golden_truth\n",
        "        self.compounds = list(golden_truth.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.compounds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        compound = self.compounds[idx]\n",
        "\n",
        "        # Check if compound exists in the embedding dictionary\n",
        "        if compound not in self.embedding_dict:\n",
        "            print(f\"Warning: Missing embedding for compound '{compound}', skipping.\")\n",
        "            return None  # Skip missing entries\n",
        "\n",
        "        # Access text embedding and image embeddings\n",
        "        data = self.embedding_dict[compound]\n",
        "        text_embedding = data[\"text_embedding\"]  # Shape: (1, 512)\n",
        "        images = data[\"images\"]\n",
        "        image_embeddings = torch.stack([img[\"image_embedding\"] for img in images])  # Shape: (num_images, 512)\n",
        "        image_ids = [img[\"image_id\"] for img in images]\n",
        "\n",
        "        # Map golden truth rank to indices\n",
        "        correct_order = self.golden_truth.get(compound, [])\n",
        "        correct_indices = [image_ids.index(img_id) for img_id in correct_order if img_id in image_ids]\n",
        "\n",
        "        return text_embedding.squeeze(0), image_embeddings, correct_indices\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def listnet_loss(pred_scores, true_indices):\n",
        "    \"\"\"\n",
        "    Compute the ListNet loss for predicted scores and true rankings.\n",
        "    \"\"\"\n",
        "    num_images = pred_scores.size(0)\n",
        "    true_distribution = torch.zeros(num_images, dtype=torch.float32)\n",
        "    for i, idx in enumerate(true_indices):\n",
        "        true_distribution[idx] = len(true_indices) - i  # Higher rank gets higher weight\n",
        "    true_distribution = true_distribution / true_distribution.sum()  # Normalize to probabilities\n",
        "    pred_prob = torch.softmax(pred_scores, dim=0)  # Predicted probabilities\n",
        "    loss = -torch.sum(true_distribution * torch.log(pred_prob + 1e-9))  # Cross-entropy loss\n",
        "    return loss\n",
        "\n",
        "def train_model(model, dataloader, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            if batch is None:  # Skip empty batches\n",
        "                continue\n",
        "\n",
        "            text_embedding, image_embeddings, true_indices = batch\n",
        "            text_embedding = text_embedding.squeeze(0)  # Shape: (512,)\n",
        "            image_embeddings = image_embeddings.squeeze(0)  # Shape: (num_images, 512)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred_scores = model(text_embedding, image_embeddings)\n",
        "            loss = listnet_loss(pred_scores, true_indices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Xehcs4Z9ihWg",
        "outputId": "14c61044-c672-4edf-d14a-acdb0db6d49e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c75a933cbe75>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_adapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-c75a933cbe75>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip empty batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-c75a933cbe75>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Access text embedding and image embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompound\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Shape: (1, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (num_images, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = RankingDataset(embedding_dict, golden_truth)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Initialize CLIP Adapter model, optimizer\n",
        "clip_adapter = CLIPAdapter(embedding_dim=512)\n",
        "optimizer = optim.Adam(clip_adapter.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(clip_adapter, dataloader, optimizer)"
      ],
      "metadata": {
        "id": "25YfOT3uvIKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}