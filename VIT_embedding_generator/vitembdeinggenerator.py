# -*- coding: utf-8 -*-
"""vitEmbdeingGenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CloWrHAaGhoLnZrgRQUaCu_KABMaarKy
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip "/content/gdrive/MyDrive/Subtask A train .zip" -d "/content/gdrive/MyDrive/Subtask_A_Train/"

!pip install transformers

import os
import torch
from transformers import ViTModel, ViTImageProcessor
from PIL import Image
from typing import List, Dict

# Load the ViT model and processor
model_name = "openai/clip-vit-large-patch14"
model = ViTModel.from_pretrained(model_name)
processor = ViTImageProcessor.from_pretrained(model_name)
model.eval()

def process_image(image_path: str):
    """
    Process an image and generate its embedding using the ViT model.
    """
    try:
        image = Image.open(image_path).convert("RGB")
        inputs = processor(images=image, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        return None

def generate_embeddings_with_names(base_folder: str, max_images: int = 5) -> Dict[str, List[Dict[str, torch.Tensor]]]:
    """
    Generate embeddings for each compound and associate them with their image names.
    """
    compound_embeddings = {}
    for compound in os.listdir(base_folder):
        compound_path = os.path.join(base_folder, compound)
        if not os.path.isdir(compound_path):
            continue

        image_data = []
        image_files = [f for f in os.listdir(compound_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:max_images]
        for image_file in image_files:
            image_path = os.path.join(compound_path, image_file)
            embedding = process_image(image_path)
            if embedding is not None:
                image_data.append({"image_name": image_file, "embedding": embedding})
        if image_data:
            compound_embeddings[compound] = image_data
    return compound_embeddings

def save_embeddings_with_names(embeddings: Dict[str, List[Dict[str, torch.Tensor]]], file_path: str):
    """
    Save the embeddings dictionary with image names to a file.
    """
    try:
        torch.save(embeddings, file_path)
        print(f"Embeddings with image names saved to {file_path}")
    except Exception as e:
        print(f"Error saving embeddings: {e}")

def load_embeddings_with_names(file_path: str) -> Dict[str, List[Dict[str, torch.Tensor]]]:
    """
    Load embeddings dictionary with image names from a file.
    """
    try:
        embeddings = torch.load(file_path)
        print(f"Embeddings with image names loaded from {file_path}")
        return embeddings
    except Exception as e:
        print(f"Error loading embeddings: {e}")
        return {}

if __name__ == "__main__":
    base_folder = "/content/gdrive/MyDrive/Subtask_A_Train/Subtask A train /literal"
    save_path = "compound_embeddings_literal.pt"

    # Generate and save embeddings
    embeddings_with_names = generate_embeddings_with_names(base_folder)
    save_embeddings_with_names(embeddings_with_names, save_path)

    # Load and validate embeddings
    loaded_embeddings = load_embeddings_with_names(save_path)
    for compound, images in loaded_embeddings.items():
        print(f"\nCompound: {compound}")
        for image_data in images:
            print(f"  Image Name: {image_data['image_name']}")
            print(f"  Embedding Size: {image_data['embedding'].size()}")