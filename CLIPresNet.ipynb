{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6LnTD8AR6i0",
        "outputId": "282e8770-ed79-417c-a101-101239b835cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch"
      ],
      "metadata": {
        "id": "BsgKYZwITPZY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载 CLIP 模型（ViT 版本）\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "qMu35qkCTUyW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_embeddings(image_paths, model, processor):\n",
        "    \"\"\"\n",
        "    提取图片嵌入及其对应的 ID。\n",
        "    :param image_paths: 图片文件路径列表\n",
        "    :param model: CLIP 模型\n",
        "    :param processor: CLIP 的预处理工具\n",
        "    :return: 包含图片 ID 和嵌入的字典\n",
        "    \"\"\"\n",
        "    image_embeddings = {}\n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # 提取图像嵌入\n",
        "        with torch.no_grad():\n",
        "            embedding = model.get_image_features(**inputs).squeeze(0)  # 输出 1x512 变为 512\n",
        "        image_embeddings[f\"image_{idx+1}\"] = embedding\n",
        "    return image_embeddings\n"
      ],
      "metadata": {
        "id": "JF-dj5YaTZWT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_embedding(text, model, processor):\n",
        "    \"\"\"\n",
        "    提取文本的嵌入。\n",
        "    :param text: 输入的文本\n",
        "    :param model: CLIP 模型\n",
        "    :param processor: CLIP 的预处理工具\n",
        "    :return: 文本的嵌入\n",
        "    \"\"\"\n",
        "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # 提取文本嵌入\n",
        "    with torch.no_grad():\n",
        "        embedding = model.get_text_features(**inputs).squeeze(0)  # 输出 1x512 变为 512\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "58Ogus4tT0O6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [\n",
        "    \"/content/02072099056.png\",\n",
        "    \"/content/16206684278.png\",\n",
        "    \"/content/32267606120.png\",\n",
        "    \"/content/52868171709.png\",\n",
        "    \"/content/55188636806.png\"\n",
        "]\n",
        "\n",
        "# 提取图片嵌入\n",
        "image_embeddings = extract_image_embeddings(image_paths, model, processor)\n",
        "print(\"Image Embeddings:\")\n",
        "for img_id, embedding in image_embeddings.items():\n",
        "    print(f\"{img_id}: {embedding.shape}\")  # 每个嵌入应为 [512]\n",
        "\n",
        "# 提取文本嵌入\n",
        "text = \"bull market\"\n",
        "text_embedding = extract_text_embedding(text, model, processor)\n",
        "print(f\"Text Embedding Shape: {text_embedding.shape}\")  # 应为 [512]\n",
        "\n",
        "# 保存嵌入\n",
        "torch.save(image_embeddings, \"image_embeddings.pt\")\n",
        "torch.save({\"bull market\": text_embedding}, \"text_embedding.pt\")\n",
        "print(\"Embeddings saved.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZuKExtoUbW0",
        "outputId": "faa774f3-dd36-46ee-ae1d-5ff05a35d58b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embeddings:\n",
            "image_1: torch.Size([512])\n",
            "image_2: torch.Size([512])\n",
            "image_3: torch.Size([512])\n",
            "image_4: torch.Size([512])\n",
            "image_5: torch.Size([512])\n",
            "Text Embedding Shape: torch.Size([512])\n",
            "Embeddings saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载图片和文本嵌入\n",
        "image_embeddings = torch.load(\"image_embeddings.pt\")  # 图片嵌入字典\n",
        "text_embedding = torch.load(\"text_embedding.pt\")[\"bull market\"]  # 文本嵌入\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAQm8OMTVemo",
        "outputId": "21813fbb-763b-468e-a3c5-8a430dcab97c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-5850df5848a4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  image_embeddings = torch.load(\"image_embeddings.pt\")  # 图片嵌入字典\n",
            "<ipython-input-37-5850df5848a4>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  text_embedding = torch.load(\"text_embedding.pt\")[\"bull market\"]  # 文本嵌入\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eaiDeRlkWmiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VectorComparisonNet(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(VectorComparisonNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, image_emb, text_emb):\n",
        "        if image_emb.dim() == 1:\n",
        "            image_emb = image_emb.unsqueeze(dim=0)\n",
        "        if text_emb.dim() == 1:\n",
        "            text_emb = text_emb.unsqueeze(dim=0)\n",
        "\n",
        "        combined = torch.cat([image_emb, text_emb], dim=-1)\n",
        "        hidden = self.relu(self.fc1(combined))\n",
        "        output = self.sigmoid(self.fc2(hidden))\n",
        "        return output"
      ],
      "metadata": {
        "id": "zQdtfKylWm9x"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义嵌入维度和隐藏层维度\n",
        "embedding_dim = 512  # CLIP 的默认嵌入维度\n",
        "hidden_dim = 128\n",
        "\n",
        "# 初始化神经网络\n",
        "model = VectorComparisonNet(embedding_dim, hidden_dim)\n",
        "\n",
        "# 如果有 GPU，加载到 GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zewFByVpWfPU",
        "outputId": "476b668a-28b6-4f1f-c90b-2b8e1db75c69"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorComparisonNet(\n",
              "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PVvt1Ju8WkN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# 将文本嵌入加载到设备\n",
        "text_embedding = text_embedding.to(device)\n",
        "\n",
        "# 遍历图片嵌入并计算相似性分数\n",
        "scores = []\n",
        "for img_path, (image_id, image_embedding) in zip(image_paths, image_embeddings.items()):\n",
        "    image_embedding = image_embedding.to(device)\n",
        "\n",
        "    # 获取文件名\n",
        "    image_name = os.path.basename(img_path)\n",
        "\n",
        "    # 通过神经网络计算相似性分数\n",
        "    with torch.no_grad():\n",
        "        similarity_score = model(image_embedding, text_embedding).item()\n",
        "\n",
        "    scores.append((image_name, similarity_score))\n",
        "\n",
        "# 根据相似性分数排序\n",
        "scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 输出图片文件名和相似性分数\n",
        "print(\"Similarity Scores (by image file names):\")\n",
        "for image_name, score in scores:\n",
        "    print(f\"{image_name}: {score:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVMnQhGYWk2B",
        "outputId": "3eaa7804-827b-498e-8d00-c0aead33c07a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Scores (by image file names):\n",
            "02072099056.png: 0.5219\n",
            "16206684278.png: 0.5068\n",
            "55188636806.png: 0.4982\n",
            "52868171709.png: 0.4954\n",
            "32267606120.png: 0.4935\n"
          ]
        }
      ]
    }
  ]
}