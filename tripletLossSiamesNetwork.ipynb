{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1DKbs5teuen",
        "outputId": "bb9a73ae-6de1-4658-9c91-2fb8f3dc0703"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR"
      ],
      "metadata": {
        "id": "qAeqEhpL4X6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This experiement takes the highest similarity image embedding as the positive example,the lowest similarityimage embedding as the negative example and the text embedding as the archor of triplet loss. The similarity is computed with the cosine similarity."
      ],
      "metadata": {
        "id": "h2rM1z2B3TAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 定义 Siamese Network 模型\n",
        "# define Siamese Network model\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# 定义三元组损失函数\n",
        "# define triplet loss\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        positive_distance = F.pairwise_distance(anchor, positive, keepdim=True)\n",
        "        negative_distance = F.pairwise_distance(anchor, negative, keepdim=True)\n",
        "        loss = F.relu(positive_distance - negative_distance + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "def get_triplets(text_embeddings, image_embeddings_batch):\n",
        "    batch_size = text_embeddings.size(0)\n",
        "    anchors = text_embeddings\n",
        "    positives = []\n",
        "    negatives = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        similarities = F.cosine_similarity(text_embeddings[i].unsqueeze(0), image_embeddings_batch[i], dim=1)\n",
        "        positive_index = similarities.argmax().item()\n",
        "\n",
        "        # Handle negative indices correctly\n",
        "        negative_indices = (similarities < similarities[positive_index]).nonzero(as_tuple=False).squeeze()\n",
        "        if negative_indices.nelement() > 0:\n",
        "            negative_index = negative_indices[torch.randint(0, len(negative_indices), (1,))].item()\n",
        "        else:\n",
        "            negative_index = torch.randint(0, image_embeddings_batch.size(0), (1,)).item()\n",
        "\n",
        "        positives.append(image_embeddings_batch[i][positive_index])\n",
        "        negatives.append(image_embeddings_batch[i][negative_index])\n",
        "\n",
        "    positives = torch.stack(positives)\n",
        "    negatives = torch.stack(negatives)\n",
        "    return anchors, positives, negatives\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 加载嵌入数据\n",
        "# load embedding data\n",
        "\n",
        "embedding_path = '/content/gdrive/MyDrive/clip_train_idiom_embeddings.pt'\n",
        "data = torch.load(embedding_path, map_location='cpu')\n",
        "text_embeddings = [item['text_embedding'].squeeze(0) for item in data]\n",
        "image_embeddings = [img['image_embedding'].squeeze(0) for item in data for img in item['images']]\n",
        "\n",
        "# 转换为张量\n",
        "# transfer to tensor\n",
        "text_embeddings_tensor = torch.stack(text_embeddings)\n",
        "image_embeddings_tensor = torch.stack(image_embeddings)\n",
        "\n",
        "# 打印嵌入张量形状\n",
        "# print the shape of embedding tensor\n",
        "print(f\"Number of text embeddings: {len(text_embeddings)}\")\n",
        "print(f\"Number of image embeddings: {len(image_embeddings)}\")\n",
        "print(f\"Image embeddings tensor shape: {image_embeddings_tensor.shape}\")\n",
        "\n",
        "# 定义数据集\n",
        "# define the dataset\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, text_embeddings, image_embeddings):\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.image_embeddings = image_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_embeddings)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.text_embeddings[index], self.image_embeddings\n",
        "\n",
        "# 数据加载器\n",
        "# load dataloader\n",
        "dataset = EmbeddingDataset(text_embeddings_tensor, image_embeddings_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 初始化模型和损失函数\n",
        "# initialize model and loss function\n",
        "embedding_dim = text_embeddings_tensor.size(1)\n",
        "model = SiameseNetwork(embedding_dim=embedding_dim)\n",
        "criterion = TripletLoss(margin=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# 训练循环\n",
        "# training loop\n",
        "num_epochs = 50\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for text_embeddings, image_embeddings_batch in dataloader:\n",
        "        anchors, positives, negatives = get_triplets(text_embeddings, image_embeddings_batch)\n",
        "\n",
        "        anchor_outputs = model(anchors)\n",
        "        positive_outputs = model(positives)\n",
        "        negative_outputs = model(negatives)\n",
        "\n",
        "        loss = criterion(anchor_outputs, positive_outputs, negative_outputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        scheduler.step(total_loss / len(dataloader))\n",
        "        # 使用学习率调度器\n",
        "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C9GFaT3UJHkl",
        "outputId": "1c4b5379-13bc-4b27-a906-101a1ea54e6a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-86c35a7f7d7f>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(embedding_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text embeddings: 38\n",
            "Number of image embeddings: 190\n",
            "Image embeddings tensor shape: torch.Size([190, 512])\n",
            "Epoch 1, Loss: 0.9954\n",
            "Epoch 2, Loss: 0.9835\n",
            "Epoch 3, Loss: 0.9699\n",
            "Epoch 4, Loss: 0.9642\n",
            "Epoch 5, Loss: 0.9543\n",
            "Epoch 6, Loss: 0.9382\n",
            "Epoch 7, Loss: 0.9102\n",
            "Epoch 8, Loss: 0.9200\n",
            "Epoch 9, Loss: 0.9173\n",
            "Epoch 10, Loss: 0.9105\n",
            "Epoch 11, Loss: 0.8921\n",
            "Epoch 12, Loss: 0.9011\n",
            "Epoch 13, Loss: 0.8875\n",
            "Epoch 14, Loss: 0.9071\n",
            "Epoch 15, Loss: 0.8902\n",
            "Epoch 16, Loss: 0.8974\n",
            "Epoch 17, Loss: 0.9194\n",
            "Epoch 18, Loss: 0.8884\n",
            "Epoch 19, Loss: 0.9058\n",
            "Epoch 20, Loss: 0.9036\n",
            "Epoch 21, Loss: 0.9062\n",
            "Epoch 22, Loss: 0.9119\n",
            "Epoch 23, Loss: 0.9142\n",
            "Epoch 24, Loss: 0.9112\n",
            "Epoch 25, Loss: 0.9034\n",
            "Epoch 26, Loss: 0.9156\n",
            "Epoch 27, Loss: 0.9017\n",
            "Epoch 28, Loss: 0.9097\n",
            "Epoch 29, Loss: 0.9062\n",
            "Epoch 30, Loss: 0.8918\n",
            "Epoch 31, Loss: 0.8977\n",
            "Epoch 32, Loss: 0.9037\n",
            "Epoch 33, Loss: 0.9003\n",
            "Epoch 34, Loss: 0.9112\n",
            "Epoch 35, Loss: 0.9092\n",
            "Epoch 36, Loss: 0.8978\n",
            "Epoch 37, Loss: 0.9075\n",
            "Epoch 38, Loss: 0.9057\n",
            "Epoch 39, Loss: 0.9175\n",
            "Epoch 40, Loss: 0.9176\n",
            "Epoch 41, Loss: 0.9016\n",
            "Epoch 42, Loss: 0.8912\n",
            "Epoch 43, Loss: 0.9086\n",
            "Epoch 44, Loss: 0.9104\n",
            "Epoch 45, Loss: 0.9097\n",
            "Epoch 46, Loss: 0.9000\n",
            "Epoch 47, Loss: 0.9036\n",
            "Epoch 48, Loss: 0.9062\n",
            "Epoch 49, Loss: 0.8898\n",
            "Epoch 50, Loss: 0.9100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过计算相似度把排在第一的image的embedding作为positive sample，后面4个图片的embedding作为negative samples，然后当排在第二的image embedding作为positive sample的时候后面3个图片的embedding作为negative samples并且从其他不相关的compound的image embedding随机选一个出来补成4个negative samples，然后当排在第三的image embedding作为positive sample的时候后面2个图片的embedding作为negative samples并且从其他不相关的compound的image embedding随机选两个出来补成4个negative samples，以此类推"
      ],
      "metadata": {
        "id": "Pcr0NFQb5NBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the get_triplets function constructs text-image triplets by calculating cosine similarity between text and image embeddings. It selects the most similar image as a positive sample and chooses lower-similarity images as negatives. If negative samples are insufficient, random negatives are drawn from remaining embeddings to ensure each triplet contains four negatives."
      ],
      "metadata": {
        "id": "jJz1WHMCJJ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义 Siamese Network 模型\n",
        "# define Siamese Network model\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "wAs_24Zt4mx0"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedTripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0, positive_weight=1.0, negative_weight=1.5):\n",
        "        super(WeightedTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.positive_weight = positive_weight\n",
        "        self.negative_weight = negative_weight\n",
        "\n",
        "    def forward(self, anchor, positive, negatives):\n",
        "        # caculate the distance between the posivtives and anchor\n",
        "\n",
        "        positive_distance = F.pairwise_distance(anchor, positive)\n",
        "\n",
        "        # caculate and average the distance between the negatives and anchor\n",
        "        negative_distance = F.pairwise_distance(anchor.unsqueeze(1), negatives).mean(dim=1)\n",
        "\n",
        "        # caculate the weighted loss\n",
        "        loss = F.relu(self.positive_weight * positive_distance - self.negative_weight * negative_distance + self.margin)\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "1OmlMiCh4s-J"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_triplets(text_embedding, image_embeddings, all_image_embeddings):\n",
        "    anchors, positives, negatives = [], [], []\n",
        "\n",
        "    # 计算文本与所有图像的相似度\n",
        "    # caculate the similarity between the text and all the image\n",
        "\n",
        "    similarities = F.cosine_similarity(text_embedding.unsqueeze(0), image_embeddings, dim=-1)\n",
        "    sorted_indices = similarities.argsort(descending=True)\n",
        "\n",
        "    for i in range(len(image_embeddings)):\n",
        "        # 正样本：当前图像嵌入\n",
        "        # positives: the embedding of the present image\n",
        "        positive_index = sorted_indices[i].item()\n",
        "        positive = image_embeddings[positive_index]\n",
        "\n",
        "        # 负样本：选择比当前正样本相似度更低的图像\n",
        "        # negatives: choose the images which have the similarity lower than the present image\n",
        "        negative_candidates = sorted_indices[i + 1:].tolist()\n",
        "\n",
        "        # 如果负样本不足 4 个，从其他图像中随机补充\n",
        "        # if the numbers of negatives are lower than 4 then we choose images from other compounds as a negative sample to make up the empty space\n",
        "        num_negatives_needed = 4 - len(negative_candidates)\n",
        "        if num_negatives_needed > 0:\n",
        "            available_negatives = [\n",
        "                idx for idx in range(all_image_embeddings.size(0))\n",
        "                if idx not in sorted_indices[:i + 1]\n",
        "            ]\n",
        "            supplement_negatives = torch.tensor(available_negatives)[\n",
        "                torch.randperm(len(available_negatives))[:num_negatives_needed]\n",
        "            ].tolist()\n",
        "            negative_candidates.extend(supplement_negatives)\n",
        "\n",
        "        # 保证每个三元组有 4 个负样本\n",
        "        # ensure every triplet group has 4 negatives\n",
        "        negative = torch.stack([all_image_embeddings[idx] for idx in negative_candidates[:4]])\n",
        "\n",
        "        # 添加到三元组列表\n",
        "        # add to triplet list\n",
        "        anchors.append(text_embedding)\n",
        "        positives.append(positive)\n",
        "        negatives.append(negative)\n",
        "\n",
        "    # 转换为张量\n",
        "    # transfer to tensor\n",
        "    anchors = torch.stack(anchors)\n",
        "    positives = torch.stack(positives)\n",
        "    negatives = torch.stack(negatives)\n",
        "\n",
        "    return anchors, positives, negatives\n",
        "\n",
        "\n",
        "# 加载嵌入数据\n",
        "# load emabedding data\n",
        "embedding_path = '/content/gdrive/MyDrive/clip_train_idiom_embeddings.pt'\n",
        "data = torch.load(embedding_path, map_location='cpu')\n",
        "\n",
        "# 去掉多余的维度\n",
        "# remove unnecessary dim\n",
        "text_embeddings = [item['text_embedding'].squeeze(0) for item in data]\n",
        "image_embeddings = [[img['image_embedding'].squeeze(0) for img in item['images']] for item in data]\n",
        "\n",
        "# 展平嵌套的 image_embeddings\n",
        "#Flatten the nested image_embeddings\n",
        "all_image_embeddings = torch.stack([img for group in image_embeddings for img in group])\n",
        "\n",
        "# 将每个图像组转换为 2D Tensor\n",
        "#change all the image group to 2D Tensor\n",
        "image_embeddings = [torch.stack(group) for group in image_embeddings]\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, text_embeddings, image_embeddings):\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.image_embeddings = image_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_embeddings)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.text_embeddings[index], self.image_embeddings[index]\n",
        "\n",
        "# 数据加载器\n",
        "# data loader\n",
        "dataloader = DataLoader(EmbeddingDataset(text_embeddings, image_embeddings), batch_size=1, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL8h_K_T4sjv",
        "outputId": "2fbacffd-4fb1-4e79-b0fc-8f8f0738c814"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-16178f0596b9>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(embedding_path, map_location='cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#初始化模型和损失函数\n",
        "#initializr the model and loss function\n",
        "embedding_dim = text_embeddings[0].shape[0]\n",
        "model = SiameseNetwork(embedding_dim=embedding_dim)\n",
        "criterion = WeightedTripletLoss(margin=1.0, positive_weight=1.0, negative_weight=1.5)\n",
        "\n",
        "# 设置学习率调度器\n",
        "# set learning rate optimizer\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "#scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
        "step_lr_scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "metadata": {
        "id": "O64X554r6C9l"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 训练循环\n",
        "# training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for text_embedding, image_embeddings_batch in dataloader:\n",
        "        # 移除 batch 维度\n",
        "        # remove batch dim\n",
        "        text_embedding = text_embedding.squeeze(0)\n",
        "        image_embeddings_batch = image_embeddings_batch.squeeze(0)\n",
        "\n",
        "        # 构建三元组\n",
        "        # set triplet group\n",
        "        anchors, positives, negatives = get_triplets(text_embedding, image_embeddings_batch, all_image_embeddings)\n",
        "\n",
        "        # 模型输出\n",
        "        # output of model\n",
        "        anchor_outputs = model(anchors)\n",
        "        positive_outputs = model(positives)\n",
        "        negative_outputs = torch.stack([model(neg) for neg in negatives])\n",
        "\n",
        "        # 计算损失\n",
        "        # caculate the loss\n",
        "        loss = criterion(anchor_outputs, positive_outputs, negative_outputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 调整学习率\n",
        "    # ajust the learning rate\n",
        "    scheduler.step(total_loss / len(dataloader))\n",
        "    step_lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhGztKwQxWEA",
        "outputId": "9da81d7a-dbe9-4cc3-b3dc-eb4c77bde853"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9563\n",
            "Epoch 2, Loss: 0.9429\n",
            "Epoch 3, Loss: 0.9285\n",
            "Epoch 4, Loss: 0.9143\n",
            "Epoch 5, Loss: 0.8970\n",
            "Epoch 6, Loss: 0.8800\n",
            "Epoch 7, Loss: 0.8593\n",
            "Epoch 8, Loss: 0.8370\n",
            "Epoch 9, Loss: 0.8117\n",
            "Epoch 10, Loss: 0.7893\n",
            "Epoch 11, Loss: 0.7604\n",
            "Epoch 12, Loss: 0.7316\n",
            "Epoch 13, Loss: 0.6985\n",
            "Epoch 14, Loss: 0.6669\n",
            "Epoch 15, Loss: 0.6301\n",
            "Epoch 16, Loss: 0.5881\n",
            "Epoch 17, Loss: 0.5457\n",
            "Epoch 18, Loss: 0.4987\n",
            "Epoch 19, Loss: 0.4492\n",
            "Epoch 20, Loss: 0.4015\n",
            "Epoch 21, Loss: 0.3527\n",
            "Epoch 22, Loss: 0.3172\n",
            "Epoch 23, Loss: 0.2989\n",
            "Epoch 24, Loss: 0.2633\n",
            "Epoch 25, Loss: 0.2326\n",
            "Epoch 26, Loss: 0.2169\n",
            "Epoch 27, Loss: 0.1900\n",
            "Epoch 28, Loss: 0.1789\n",
            "Epoch 29, Loss: 0.1531\n",
            "Epoch 30, Loss: 0.1499\n",
            "Epoch 31, Loss: 0.1411\n",
            "Epoch 32, Loss: 0.1360\n",
            "Epoch 33, Loss: 0.1366\n",
            "Epoch 34, Loss: 0.1348\n",
            "Epoch 35, Loss: 0.1241\n",
            "Epoch 36, Loss: 0.1297\n",
            "Epoch 37, Loss: 0.1260\n",
            "Epoch 38, Loss: 0.1206\n",
            "Epoch 39, Loss: 0.1232\n",
            "Epoch 40, Loss: 0.1261\n",
            "Epoch 41, Loss: 0.1229\n",
            "Epoch 42, Loss: 0.1172\n",
            "Epoch 43, Loss: 0.1200\n",
            "Epoch 44, Loss: 0.1148\n",
            "Epoch 45, Loss: 0.1082\n",
            "Epoch 46, Loss: 0.1179\n",
            "Epoch 47, Loss: 0.1093\n",
            "Epoch 48, Loss: 0.1177\n",
            "Epoch 49, Loss: 0.1116\n",
            "Epoch 50, Loss: 0.1051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add early stopping to improve traning process"
      ],
      "metadata": {
        "id": "LzVOtoneAG8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Early Stopping with patience=10\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, current_loss):\n",
        "        if self.best_loss is None or current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "#early_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n"
      ],
      "metadata": {
        "id": "jmxG5ZEe9odi"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Early Stopping with patience=8 which works better\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=8, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, current_loss):\n",
        "        if self.best_loss is None or current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "early_stopping = EarlyStopping(patience=8, min_delta=0.0001)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yAPLAG_AC3pA"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练循环\n",
        "# taining loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for text_embedding, image_embeddings_batch in dataloader:\n",
        "        # 移除 batch 维度\n",
        "        # remove batch dim\n",
        "        text_embedding = text_embedding.squeeze(0)\n",
        "        image_embeddings_batch = image_embeddings_batch.squeeze(0)\n",
        "\n",
        "        # 构建三元组\n",
        "        # set up triplet group\n",
        "        anchors, positives, negatives = get_triplets(text_embedding, image_embeddings_batch, all_image_embeddings)\n",
        "\n",
        "        # 模型输出\n",
        "        # output of model\n",
        "        anchor_outputs = model(anchors)\n",
        "        positive_outputs = model(positives)\n",
        "        negative_outputs = torch.stack([model(neg) for neg in negatives])\n",
        "\n",
        "        # 计算损失\n",
        "        # caculate the loss\n",
        "        loss = criterion(anchor_outputs, positive_outputs, negative_outputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 调整学习率，传入损失值\n",
        "    # adjust learning rate\n",
        "    scheduler.step(total_loss / len(dataloader))\n",
        "\n",
        "    # Early stopping 检查\n",
        "    # check early stopping\n",
        "    if early_stopping.step(total_loss / len(dataloader)):\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZUuOtn_wu1",
        "outputId": "b35406ad-05d1-4e58-da30-5a6eb14ad356"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0258\n",
            "Epoch 2, Loss: 0.0238\n",
            "Epoch 3, Loss: 0.0265\n",
            "Epoch 4, Loss: 0.0226\n",
            "Epoch 5, Loss: 0.0247\n",
            "Epoch 6, Loss: 0.0215\n",
            "Epoch 7, Loss: 0.0229\n",
            "Epoch 8, Loss: 0.0204\n",
            "Epoch 9, Loss: 0.0277\n",
            "Epoch 10, Loss: 0.0246\n",
            "Epoch 11, Loss: 0.0210\n",
            "Epoch 12, Loss: 0.0241\n",
            "Epoch 13, Loss: 0.0206\n",
            "Epoch 14, Loss: 0.0244\n",
            "Epoch 15, Loss: 0.0219\n",
            "Epoch 16, Loss: 0.0183\n",
            "Epoch 17, Loss: 0.0242\n",
            "Epoch 18, Loss: 0.0204\n",
            "Epoch 19, Loss: 0.0194\n",
            "Epoch 20, Loss: 0.0193\n",
            "Epoch 21, Loss: 0.0215\n",
            "Epoch 22, Loss: 0.0243\n",
            "Epoch 23, Loss: 0.0222\n",
            "Early stopping at epoch 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型的权重\n",
        "# save model weights\n",
        "\n",
        "torch.save(model.state_dict(), '/content/gdrive/MyDrive/siamese_model_weights.pth')\n",
        "print(\"Model saves to 'siamese_model_weights.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tKwYhnEON_C",
        "outputId": "8925a707-2fca-4ad2-c259-d143f53c0066"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saves to 'siamese_model_weights.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 加载模型权重\n",
        "# load model weights\n",
        "\n",
        "model_weights = torch.load('/content/gdrive/MyDrive/siamese_model_weights.pth')\n",
        "\n",
        "# 查看 state_dict 的 keys\n",
        "# check keys of state_dict\n",
        "print(model_weights.keys())\n",
        "\n",
        "# 检查某些层的权重形状\n",
        "# check shape\n",
        "\n",
        "for key, value in model_weights.items():\n",
        "    print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tql_ZBrJXK9C",
        "outputId": "e653f9c3-d0b7-4f45-ed12-e502e58a6f7a"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'output_layer.weight', 'output_layer.bias'])\n",
            "fc1.weight: torch.Size([256, 512])\n",
            "fc1.bias: torch.Size([256])\n",
            "fc2.weight: torch.Size([128, 256])\n",
            "fc2.bias: torch.Size([128])\n",
            "fc3.weight: torch.Size([64, 128])\n",
            "fc3.bias: torch.Size([64])\n",
            "output_layer.weight: torch.Size([32, 64])\n",
            "output_layer.bias: torch.Size([32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-117-e3f1149850e1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_weights = torch.load('/content/gdrive/MyDrive/siamese_model_weights.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import csv\n",
        "\n",
        "\n",
        "# 定义孪生网络模型\n",
        "# defien model\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, 32)  # 最后一层输出 32 维度的向量\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# 加载权重\n",
        "# load weights\n",
        "model = SiameseNetwork(embedding_dim=512)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/siamese_model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "# 加载 CLIP 嵌入文件\n",
        "#load CLIP embedding file\n",
        "\n",
        "clip_embeddings = torch.load('/content/gdrive/MyDrive/clip_dev_embeddings.pt')\n",
        "\n",
        "\n",
        "# 计算文本和图像的相似度，并生成排序\n",
        "#caculate similarity and generate ranking\n",
        "results = []\n",
        "for sample in clip_embeddings:\n",
        "    compound_name = sample['compound_name']\n",
        "    text_embedding = sample['text_embedding']\n",
        "    image_embeddings = sample['images']\n",
        "\n",
        "    text_feature = model(text_embedding)\n",
        "\n",
        "    # 计算每个图片的相似度\n",
        "    #caculate similarity of each image\n",
        "    image_scores = []\n",
        "    for image in image_embeddings:\n",
        "        image_id = image['image_id']\n",
        "        image_feature = model(image['image_embedding'])\n",
        "        similarity = F.cosine_similarity(text_feature, image_feature).item()\n",
        "        image_scores.append((image_id, similarity))\n",
        "\n",
        "    # 按相似度排序\n",
        "    # rank according to similarity\n",
        "    image_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # 仅保留排序后的图片 ID\n",
        "    #only save the iameg ID\n",
        "    ranked_images = [image_id for image_id, _ in image_scores]\n",
        "\n",
        "    #save result\n",
        "    results.append([compound_name] + ranked_images)\n",
        "\n",
        "# 保存为 TSV 文件\n",
        "# save to TSV\n",
        "with open('/content/submission.tsv', 'w', newline='') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    writer.writerow(['compound', 'expected_order'])\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(\"TSV file saved：/content/submission.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAQAbeDWVwZ6",
        "outputId": "4fb954c4-3130-4199-832b-4b27600875d6"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSV 文件已生成：/content/submission.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-124-9aaebc9c955d>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/gdrive/MyDrive/siamese_model_weights.pth'))\n",
            "<ipython-input-124-9aaebc9c955d>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  clip_embeddings = torch.load('/content/gdrive/MyDrive/clip_dev_embeddings.pt')\n"
          ]
        }
      ]
    }
  ]
}